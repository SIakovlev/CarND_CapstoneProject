
(carnd-advdl-odlab) C:\Users\UK000044\git\CarND_CapstoneProject\training>python main.py
TensorFlow Version: 1.4.0
2018-07-21 08:46:29.180205: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2018-07-21 08:46:29.468124: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties:
name: Quadro P3000 major: 6 minor: 1 memoryClockRate(GHz): 1.215
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.97GiB
2018-07-21 08:46:29.474540: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro P3000, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-07-21 08:46:29.770398: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro P3000, pci bus id: 0000:01:00.0, compute capability: 6.1)
Default GPU Device: /device:GPU:0
2018-07-21 08:46:29.789187: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro P3000, pci bus id: 0000:01:00.0, compute capability: 6.1)
Selected 285 total images (214 for training, propn=0.750000)
2018-07-21 08:46:50.161614: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-07-21 08:46:50.761822: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-07-21 08:46:50.779175: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
......................................................................................................................................................................................................................
Epoch: 0 Loss/batch: 10990.699550129879 time so far: 0:01:37.977454......................................................................................................................................................................................................................
Epoch: 1 Loss/batch: 1.2636495460973722 time so far: 0:03:13.533040......................................................................................................................................................................................................................
Epoch: 2 Loss/batch: 1.1977123785241741 time so far: 0:04:49.263716......................................................................................................................................................................................................................
Epoch: 3 Loss/batch: 1.1466945281652647 time so far: 0:06:24.782546......................................................................................................................................................................................................................
Epoch: 4 Loss/batch: 1.1069990485628074 time so far: 0:08:00.293021......................................................................................................................................................................................................................
Epoch: 5 Loss/batch: 1.075904991303649 time so far: 0:09:35.744654......................................................................................................................................................................................................................
Epoch: 6 Loss/batch: 1.051352978866791 time so far: 0:11:11.239064......................................................................................................................................................................................................................
Epoch: 7 Loss/batch: 1.03179325232996 time so far: 0:12:46.694393......................................................................................................................................................................................................................
Epoch: 8 Loss/batch: 1.01606338854148 time so far: 0:14:22.426550......................................................................................................................................................................................................................
Epoch: 9 Loss/batch: 1.0032912417549953 time so far: 0:15:58.115593......................................................................................................................................................................................................................
Epoch: 10 Loss/batch: 0.992821235801572 time so far: 0:17:33.835748......................................................................................................................................................................................................................
Epoch: 11 Loss/batch: 0.98415821651432 time so far: 0:19:09.496767......................................................................................................................................................................................................................
Epoch: 12 Loss/batch: 0.9769261727266223 time so far: 0:20:44.911590......................................................................................................................................................................................................................
Epoch: 13 Loss/batch: 0.9708377482178056 time so far: 0:22:20.343931......................................................................................................................................................................................................................
Epoch: 14 Loss/batch: 0.9656716309418188 time so far: 0:23:55.875465......................................................................................................................................................................................................................
Epoch: 15 Loss/batch: 0.9612561288838074 time so far: 0:25:31.317907......................................................................................................................................................................................................................
Epoch: 16 Loss/batch: 0.9574567196525146 time so far: 0:27:06.716057......................................................................................................................................................................................................................
Epoch: 17 Loss/batch: 0.9541674234042657 time so far: 0:28:42.119775......................................................................................................................................................................................................................
Epoch: 18 Loss/batch: 0.9513038742208035 time so far: 0:30:17.560029......................................................................................................................................................................................................................
Epoch: 19 Loss/batch: 0.9487982718186958 time so far: 0:31:52.933102
Training Finished. Saving test images to: ./runs\1532161121.6105442
Test set complete, 39 of 71 correct (proportion=0.549296)


(carnd-advdl-odlab) C:\Users\UK000044\git\CarND_CapstoneProject\training>